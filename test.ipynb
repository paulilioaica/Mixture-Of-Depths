{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, top_k, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_hidden, 1)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        #get scores for each token\n",
    "        scores = self.linear(x)\n",
    "        #get top k tokens \n",
    "        top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=1)\n",
    "        #get topk from x\n",
    "        x_top_k = x.gather(1, top_k_indices.expand(-1, -1, x.size(-1)))\n",
    "\n",
    "        return x_top_k, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = Router(top_k=8, num_hidden=128)\n",
    "x_topk, indices = router(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Positional encoding definition\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, 1, d_model)\n",
    "        self.pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.pe = self.pe.to(x.device)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads, seq_len, d_k) -> None:\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.W_q = nn.Linear(num_hidden, num_heads * num_hidden)\n",
    "        self.W_k = nn.Linear(num_hidden, num_heads * num_hidden)\n",
    "        self.W_v = nn.Linear(num_hidden, num_heads * num_hidden)\n",
    "        self.W_o = nn.Linear(num_heads * num_hidden, num_hidden)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.mask = self.get_mask(self.seq_len)\n",
    "    \n",
    "    def get_mask(self, size):\n",
    "        device = next(self.parameters()).device\n",
    "        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)  \n",
    "        print(mask)\n",
    "        return mask.unsqueeze(0).unsqueeze(0)  \n",
    "\n",
    "    def forward(self, query, key, values, dropout=0.1, mask=None):\n",
    "        # Reshaping expanded to n_heads\n",
    "        seq_len, num_hidden = query.shape[1], query.shape[2]\n",
    "        query = self.W_q(query).view(-1, self.num_heads, seq_len, num_hidden)\n",
    "        key = self.W_k(key).view(-1, self.num_heads, seq_len, num_hidden)\n",
    "        values = self.W_v(values).view(-1, self.num_heads, seq_len, num_hidden)\n",
    "\n",
    "        # Q * K_T\n",
    "        QK_T = torch.matmul(query,  key.mT)\n",
    "\n",
    "        # QK_T / sqrt(dk)\n",
    "        QK_T = QK_T / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask:\n",
    "            self.mask = self.mask.to(query.device)\n",
    "            print(self.mask.shape)\n",
    "            print(QK_T.shape)\n",
    "            QK_T = QK_T.masked_fill(self.mask == 1, float('-inf'))\n",
    "\n",
    "        # softmax(QK_T / sqrt(d_k)\n",
    "        attention_scores = self.softmax(QK_T)\n",
    "        \n",
    "        #dropout\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        output = torch.matmul(attention_scores, values)  \n",
    "        # Reshape and apply output linear layer  \n",
    "        output = output.transpose(1, 2).contiguous().view(-1, seq_len, self.num_heads * num_hidden)  \n",
    "        output = self.W_o(output)  \n",
    "          \n",
    "        return output  \n",
    "\n",
    "# Feed forward definition\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ffn_hidden) -> None:\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_ffn_hidden = num_ffn_hidden\n",
    "\n",
    "        self.W_1 = nn.Linear(num_hidden, num_ffn_hidden)\n",
    "        self.W_2 = nn.Linear(num_ffn_hidden, num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W_2(F.relu(self.W_1(x)))\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, n_heads, seq_len, num_hidden) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.decoders = nn.ModuleList([TransformerDecoderLayer(num_hidden, n_heads, seq_len) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        topk_seq_len = seq_len // 2\n",
    "        self.multihead_attention_masked = MultiHeadAttention(num_hidden=num_hidden, num_heads=num_heads, seq_len=topk_seq_len, d_k=1)\n",
    "        self.multihead_attention = MultiHeadAttention(num_hidden=num_hidden, num_heads=num_heads, seq_len=topk_seq_len, d_k=1)\n",
    "        \n",
    "        self.feed_forward = FeedForward(num_hidden=num_hidden, num_ffn_hidden= 2 * num_hidden)\n",
    "        self.layer_norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.layer_norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.layer_norm3 = nn.LayerNorm(num_hidden)\n",
    "        self.router = Router(top_k=seq_len//2, num_hidden=num_hidden)\n",
    "\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        # basically route some tokens in or around the att + lin + norm + feedforward block\n",
    "\n",
    "        #get topk \n",
    "        x_topk, x_topk_indices = self.router(x_in)\n",
    "        print(0)\n",
    "        print(x_topk.shape)\n",
    "        # masked attention\n",
    "        x_topk_att = self.multihead_attention_masked(x_topk, x_topk, x_topk, mask=True)\n",
    "        print(1)\n",
    "        print(x_topk_att.shape)\n",
    "        #add and norm\n",
    "        x_topk = x_topk + x_topk_att\n",
    "        x_topk = self.layer_norm1(x_topk)\n",
    "        print(2)\n",
    "        print(x_topk)\n",
    "        # attention\n",
    "        x_topk_att = self.multihead_attention(x_topk, x_topk, x_topk)\n",
    "\n",
    "        #add and norm\n",
    "        x_topk = x_topk + x_topk_att\n",
    "        x_topk = self.layer_norm2(x_topk)\n",
    "\n",
    "        #feed forward\n",
    "        x_topk_forward = self.feed_forward(x_topk)\n",
    "\n",
    "        #add and norm\n",
    "        x_topk = x_topk_forward + x_topk\n",
    "\n",
    "        #now you want to insert the tokens back into the original sequence, index by x_topk_indices \n",
    "        x_in.scatter_(1, x_topk_indices.expand(-1, -1, x_in.size(-1)), x_topk)\n",
    "\n",
    "        x = self.layer_norm3(x_in)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, decoder_layers_num, num_hidden, num_heads, seq_len, vocab_size, embedding_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder = TransformerDecoder(decoder_layers_num, num_heads, seq_len, num_hidden)\n",
    "        self.pos_enc = PositionalEncoding(embedding_dim, max_len=seq_len)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        #pos encodings\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        #forward pass\n",
    "        dec_output = self.decoder(x)\n",
    "        output = self.linear(dec_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, top_k, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_hidden, 1)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        #get scores for each token\n",
    "        scores = self.linear(x)\n",
    "        #get top k tokens \n",
    "        top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=1)\n",
    "        #get topk from x\n",
    "        x_top_k = x.gather(1, top_k_indices.expand(-1, -1, x.size(-1)))\n",
    "\n",
    "        return x_top_k, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(decoder_layers_num=1, num_hidden=128, num_heads=4, seq_len=16, vocab_size=100, embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "batch_size = 1\n",
    "seq_len = 16\n",
    "num_features = 128\n",
    "\n",
    "input_data = torch.randint(1, 100, (batch_size, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 8, 128])\n",
      "torch.Size([1, 1, 8, 8])\n",
      "1\n",
      "torch.Size([1, 8, 128])\n",
      "2\n",
      "tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.5353, -1.1913,  1.5220,  ..., -1.1421, -1.2691, -0.5198],\n",
      "         [-2.5160,  1.0288, -2.5979,  ...,  1.2082, -0.3471,  0.0546],\n",
      "         ...,\n",
      "         [-0.6163, -2.5728,  0.2787,  ...,  0.9897, -0.4610,  0.5872],\n",
      "         [-0.1885, -1.8652, -1.0006,  ...,  1.1170, -0.5926, -0.0821],\n",
      "         [-0.1796,  1.2794, -1.6767,  ..., -0.1821, -0.5704,  1.0722]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7462,     nan,  1.1272,     nan,  2.9755, -0.7132, -3.7785,  2.7939,\n",
       "         -4.6459,     nan, -0.9408,     nan,     nan,     nan,     nan,     nan]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(input_data).sum(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
